{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在WNUT2017上的三次实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p,r,f1 on WNUT2017 with 0-shot:\n",
      "0.36879432624113473,0.6046511627906976,0.45814977973568277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52, 89, 52, 34)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'WNUT2017'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "## TODO\n",
    "output_filepath = output_directory+'/result1.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "predict_data,actual_data\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        predict = json.loads(predict)\n",
    "    except:\n",
    "        continue\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "    #     actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        key2 = key1.lower().replace(' ','_')\n",
    "        # if key2 not in actual.keys():\n",
    "        #     continue\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key2 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key2]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        key1 = key2[0].upper()+key2[1:]\n",
    "        key1 = key1.replace('_',' ')\n",
    "        # if key1 not in predict.keys():\n",
    "            # continue\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key1 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key1]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "# print(TP1,FP,TP2,FN)\n",
    "print(f'p,r,f1 on {dataset_name} with 0-shot:',f'{p},{r},{f1}',sep='\\n')\n",
    "TP1,FP,TP2,FN\n",
    "# p,r,f1\n",
    "## FN过大，说明gpt倾向于把什么东西都当作NE了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 52 47 43\n",
      "p,r,f1 on WNUT2017 with 3-shot:\n",
      "0.47474747474747475,0.5222222222222223,0.4973544973544973\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'WNUT2017'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "## TODO\n",
    "output_filepath = output_directory+'/result2.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        # predict = json.loads(predict)\n",
    "        predict = eval(predict)\n",
    "    except:\n",
    "        # print(i)\n",
    "        continue\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "    #     actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        # if key1 not in actual.keys():\n",
    "            # continue\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        # if key2 not in predict.keys():\n",
    "            # continue\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(TP1,FP,TP2,FN)\n",
    "print(f'p,r,f1 on {dataset_name} with 3-shot:',f'{p},{r},{f1}',sep='\\n')\n",
    "# TP1,FP,TP2,FN\n",
    "# p,r,f1\n",
    "## FN过大，说明gpt倾向于把什么东西都当作NE了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 54 50 40\n",
      "p,r,f1 on WNUT2017 with 5-shot:\n",
      "0.4807692307692308,0.5555555555555556,0.5154639175257731\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'WNUT2017'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "## TODO\n",
    "output_filepath = output_directory+'/result3.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        # predict = json.loads(predict)\n",
    "        predict = eval(predict)\n",
    "    except:\n",
    "        # print(i)\n",
    "        continue\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "    #     actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        # if key1 not in actual.keys():\n",
    "            # continue\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        # if key2 not in predict.keys():\n",
    "            # continue\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(TP1,FP,TP2,FN)\n",
    "print(f'p,r,f1 on {dataset_name} with 5-shot:',f'{p},{r},{f1}',sep='\\n')\n",
    "# TP1,FP,TP2,FN\n",
    "# p,r,f1\n",
    "## FN过大，说明gpt倾向于把什么东西都当作NE了。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在Twitter上的三次实验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1,FP,TP2,FN:\n",
      "106,183,108,55\n",
      "p,r,f1 on Twitter with 0-shot:\n",
      "0.36678200692041524,0.6625766871165644,0.47217983089296756\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Twitter'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "## TODO\n",
    "output_filepath = output_directory+'/result1.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "predict_data,actual_data\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        predict = json.loads(predict)\n",
    "    except:\n",
    "        continue\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "    #     actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(f'TP1,FP,TP2,FN:\\n{TP1},{FP},{TP2},{FN}')\n",
    "print(f'p,r,f1 on {dataset_name} with 0-shot:',f'{p},{r},{f1}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1,FP,TP2,FN:\n",
      "100,129,102,61\n",
      "p,r,f1 on Twitter with 3-shot:\n",
      "0.4366812227074236,0.6257668711656442,0.5143981037873822\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Twitter'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "## TODO\n",
    "output_filepath = output_directory+'/result2.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        predict = eval(predict)\n",
    "    except:\n",
    "        continue\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "    #     actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(f'TP1,FP,TP2,FN:\\n{TP1},{FP},{TP2},{FN}')\n",
    "print(f'p,r,f1 on {dataset_name} with 3-shot:',f'{p},{r},{f1}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1,FP,TP2,FN:\n",
      "91,141,92,71\n",
      "p,r,f1 on Twitter with 5-shot:\n",
      "0.3922413793103448,0.5644171779141104,0.462835503220278\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Twitter'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "## TODO\n",
    "output_filepath = output_directory+'/result3.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        predict = eval(predict)\n",
    "    except:\n",
    "        continue\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "    #     actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(f'TP1,FP,TP2,FN:\\n{TP1},{FP},{TP2},{FN}')\n",
    "print(f'p,r,f1 on {dataset_name} with 5-shot:',f'{p},{r},{f1}',sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在Bio-NER上的三次实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1,FP,TP2,FN:\n",
      "78,82,86,130\n",
      "p,r,f1 on Bio-NER with 0-shot:\n",
      "0.4875,0.39814814814814814,0.4383167799268165\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Bio-NER'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "output_filepath = output_directory+'/result1.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        predict = eval(predict)\n",
    "        # print(predict)\n",
    "    except Exception as e:#如果有文本，通常内容为“当前句子找不出NE”,所以predict就是一个空字典。\n",
    "        predict = dict()\n",
    "    finally:\n",
    "        if 'cell-type' in predict.keys():\n",
    "            predict['cell_type'] = predict.pop('cell-type')\n",
    "        elif 'cell-line' in predict.keys():\n",
    "            predict['cell_line'] = predict.pop('cell-line')\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "        # actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(f'TP1,FP,TP2,FN:\\n{TP1},{FP},{TP2},{FN}')\n",
    "print(f'p,r,f1 on {dataset_name} with 0-shot:',f'{p},{r},{f1}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1,FP,TP2,FN:\n",
      "92,89,99,117\n",
      "p,r,f1 on Bio-NER with 3-shot:\n",
      "0.5082872928176796,0.4583333333333333,0.482019528459157\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Bio-NER'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "output_filepath = output_directory+'/result2.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        predict = eval(predict)\n",
    "        # print(predict)\n",
    "    except Exception as e:#如果有文本，通常内容为“当前句子找不出NE”,所以predict就是一个空字典。\n",
    "        predict = dict()\n",
    "    finally:\n",
    "        if 'cell-type' in predict.keys():\n",
    "            predict['cell_type'] = predict.pop('cell-type')\n",
    "        elif 'cell-line' in predict.keys():\n",
    "            predict['cell_line'] = predict.pop('cell-line')\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "        # actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(f'TP1,FP,TP2,FN:\\n{TP1},{FP},{TP2},{FN}')\n",
    "print(f'p,r,f1 on {dataset_name} with 3-shot:',f'{p},{r},{f1}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP1,FP,TP2,FN:\n",
      "104,93,110,106\n",
      "p,r,f1 on Bio-NER with 5-shot:\n",
      "0.5279187817258884,0.5092592592592593,0.5184211718856212\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Bio-NER'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "output_filepath = output_directory+'/result3.json'\n",
    "\n",
    "with open(temp_filepath,'r',encoding='utf-8') as f:\n",
    "    actual_data = json.load(f)\n",
    "with open(output_filepath,'r',encoding='utf-8') as f:\n",
    "    predict_data = json.load(f)\n",
    "\n",
    "TP1,FP,TP2,FN = 0,0,0,0\n",
    "for i,predict in enumerate(predict_data):\n",
    "    try:\n",
    "        predict = eval(predict)\n",
    "        # print(predict)\n",
    "    except Exception as e:#如果有文本，通常内容为“当前句子找不出NE”,所以predict就是一个空字典。\n",
    "        predict = dict()\n",
    "    finally:\n",
    "        if 'cell-type' in predict.keys():\n",
    "            predict['cell_type'] = predict.pop('cell-type')\n",
    "        elif 'cell-line' in predict.keys():\n",
    "            predict['cell_line'] = predict.pop('cell-line')\n",
    "    text = actual_data[i]['TEXT']\n",
    "    actual = actual_data[i]['NEs']\n",
    "##TODO 做个处理\n",
    "    for key1 in predict:\n",
    "        predict[key1] = [entity for entity in predict[key1] if entity in text]\n",
    "    # for key2 in actual:\n",
    "        # actual[key2] = list(set(actual[key2]))\n",
    "\n",
    "    for key1 in predict.keys():\n",
    "        for predict_entity in predict[key1]:\n",
    "            if key1 not in actual.keys():\n",
    "                FP+=1\n",
    "                continue\n",
    "            if predict_entity in actual[key1]:\n",
    "                TP1+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "    for key2 in actual.keys():\n",
    "        for actual_entity in actual[key2]:\n",
    "            if key2 not in predict.keys():\n",
    "                FN+=1\n",
    "                continue\n",
    "            if actual_entity not in predict[key2]:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TP2+=1\n",
    "p = TP1/(TP1 + FP)\n",
    "r = TP2/(TP2 + FN)\n",
    "f1 = 2*r*p/(r+p)\n",
    "print(f'TP1,FP,TP2,FN:\\n{TP1},{FP},{TP2},{FN}')\n",
    "print(f'p,r,f1 on {dataset_name} with 5-shot:',f'{p},{r},{f1}',sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
