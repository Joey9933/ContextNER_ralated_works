{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools,json\n",
    "from collections import defaultdict\n",
    "\n",
    "### 从train数据集中选择某些句子，作为之后few-shot所使用的例句.输出包括了连续的文段和对应的所有命名实体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(seq):\n",
    "    \"\"\"\n",
    "    Gets entities from sequence.\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        seq = ['B-PER', 'I-PER', 'O', 'B-LOC', 'I-PER']\n",
    "        get_entity_bio(seq)\n",
    "        #output\n",
    "        [['PER', 0,1], ['LOC', 3, 3], ['PER', 4, 4]]\n",
    "    \"\"\"\n",
    "    # if any(isinstance(s, list) for s in seq):\n",
    "    #     seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "    if any(isinstance(s,list) for s in seq):\n",
    "        seq = list(itertools.chain(*seq))\n",
    "\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    in_chunk = False\n",
    "    # for i, chunk in enumerate(seq + ['O']):\n",
    "    for i, chunk in enumerate(seq):\n",
    "        tag = chunk[0]\n",
    "        type_ = chunk.split('-')[-1]\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_) and in_chunk:\n",
    "            chunks.append((prev_type, begin_offset, i - 1))\n",
    "            in_chunk = False\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            in_chunk = True\n",
    "            begin_offset = i\n",
    "\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "    # don't forget if the last label is B- or I-:\n",
    "    if in_chunk:\n",
    "        chunks.append((prev_type,begin_offset,i-1))\n",
    "        in_chunk = False\n",
    "    return sorted(chunks,key=lambda x:x[1])\n",
    "    # return sorted(list(set(chunks)), key=lambda x: x[1])\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"\n",
    "    Checks if a chunk ended between the previous and current word.\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_end: boolean.\n",
    "    \"\"\"\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"\n",
    "    Checks if a chunk started between the previous and current word.\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_start: boolean.\n",
    "    \"\"\"\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "\n",
    "    # if tag == 'B': chunk_start = True\n",
    "    # if tag == 'S': chunk_start = True\n",
    "    # if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    # if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    # if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    # if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    # if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    # if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "    # if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "    #     chunk_start = True\n",
    "    return chunk_start\n",
    "\n",
    "def load_file(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        data = f.read().splitlines()\n",
    "    label_chunk,labels = [],[]\n",
    "    word_chunk,words = [],[]\n",
    "\n",
    "    for line in data:\n",
    "        if line:\n",
    "            word,label = line.split(' ')\n",
    "            word_chunk.append(word)\n",
    "            label_chunk.append(label)\n",
    "        else:\n",
    "            if not word_chunk:\n",
    "                continue\n",
    "            else:\n",
    "                words.append(word_chunk)\n",
    "                labels.append(label_chunk)\n",
    "                word_chunk,label_chunk = [],[]\n",
    "    if word_chunk:\n",
    "        words.append(word_chunk)\n",
    "        labels.append(label_chunk)\n",
    "    return [words,labels]\n",
    "\n",
    "def get_NE_word(words,labels):\n",
    "    # get a list of tuple like (<named entity>,<type>), the same phrase could occur repeatedly. \n",
    "    # if an identical word with different label, that will be recogized as different entity.\n",
    "    if any(isinstance(x,list) for x in words):\n",
    "        words = list(itertools.chain(*words))\n",
    "    entities= get_entities(labels)\n",
    "    entity_words = []\n",
    "    for entity in entities:\n",
    "        start,end = entity[1],entity[2]\n",
    "        word = ' '.join(words[start:(end+1)])\n",
    "        t = (word,entity[0])\n",
    "        if word!='':\n",
    "            entity_words.append(t)\n",
    "    return entity_words\n",
    "\n",
    "def reshape_data_new(word_list,label_list,temp_filepath):\n",
    "    result = []\n",
    "    count = len(result)\n",
    "    sentences = [' '.join(each) for each in word_list]\n",
    "    for i in range(len(sentences)):\n",
    "        if count>=100:\n",
    "            break\n",
    "        text = sentences[i]\n",
    "        if len(text)<=30:\n",
    "            continue\n",
    "        entity_res = get_NE_word(words=word_list[i],labels=label_list[i])\n",
    "        entity_dict = defaultdict(list)\n",
    "        for each in entity_res:\n",
    "            entity_dict[each[1]].append(each[0])\n",
    "        result.append({'TEXT':text,'NEs':entity_dict})\n",
    "        count = len(result)\n",
    "\n",
    "\n",
    "    with open(temp_filepath,'w',encoding='utf-8') as f:\n",
    "        json.dump(result,f,indent=4,ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TEXT': 'IL-2 gene expression and NF-kappa B activation through CD28 requires reactive oxygen production by 5-lipoxygenase .',\n",
       "  'NEs': defaultdict(list,\n",
       "              {'DNA': ['IL-2 gene'],\n",
       "               'protein': ['NF-kappa B', 'CD28', '5-lipoxygenase']})},\n",
       " {'TEXT': 'This differential effect of E1A expression on the cytolytic phenotypes of infected and stably transfected human cells suggests that human NK cells provide an effective immunologic barrier against the in vivo survival and neoplastic progression of E1A-immortalized cells that may emerge from the reservoir of persistently infected cells in the human host .',\n",
       "  'NEs': defaultdict(list,\n",
       "              {'protein': ['E1A'],\n",
       "               'cell_type': ['human cells',\n",
       "                'human NK cells',\n",
       "                'persistently infected cells'],\n",
       "               'cell_line': ['E1A-immortalized cells']})},\n",
       " {'TEXT': 'In addition , when used together IL-2 and IL-12 synergized in the induction of IFN-gamma and GM-CSF and this synergy was attributed to an increased accumulation and stability of the IFN-gamma and GM-CSF mRNAs .',\n",
       "  'NEs': defaultdict(list,\n",
       "              {'protein': ['IL-2', 'IL-12', 'IFN-gamma', 'GM-CSF'],\n",
       "               'RNA': ['IFN-gamma and GM-CSF mRNAs']})},\n",
       " {'TEXT': 'In contrast , IL-12 induction of IFN-gamma cytoplasmic mRNA appears to only partially depend on activation of protein kinase C .',\n",
       "  'NEs': defaultdict(list,\n",
       "              {'protein': ['IL-12', 'protein kinase C'],\n",
       "               'RNA': ['IFN-gamma cytoplasmic mRNA']})},\n",
       " {'TEXT': 'Northern blot analysis indicated the expression of human PEBP2 alpha A , alpha B ( AML1 ) , and beta genes in Jurkat cells .',\n",
       "  'NEs': defaultdict(list,\n",
       "              {'protein': ['PEBP2 alpha A'],\n",
       "               'DNA': ['alpha B', 'AML1', 'beta genes'],\n",
       "               'cell_line': ['Jurkat cells']})}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_name = 'WNUT2017'\n",
    "# dataset_name = 'Twitter'\n",
    "dataset_name = 'Bio-NER'\n",
    "output_directory = f'./output/{dataset_name}'\n",
    "file_path = f'./data/{dataset_name}/test.txt'\n",
    "temp_filepath = output_directory+'/query.json'\n",
    "test_words,test_labels = load_file(file_path)\n",
    "query_data = reshape_data_new(test_words,test_labels,temp_filepath)\n",
    "\n",
    "file_path = f'./output/{dataset_name}/shot.txt'\n",
    "shot_prompt_filepath = output_directory+'/5shot.json'\n",
    "test_words,test_labels = load_file(file_path)\n",
    "reshape_data_new(test_words,test_labels,shot_prompt_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
